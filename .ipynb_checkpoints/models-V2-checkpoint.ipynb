{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "240842e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "from scipy.stats import loguniform\n",
    "from statistics import mean\n",
    "from scipy import stats\n",
    "import seaborn as sns\n",
    "#import scikit_posthocs\n",
    "import warnings \n",
    "import sklearn \n",
    "\n",
    "from sklearn.model_selection import train_test_split, RepeatedStratifiedKFold, RandomizedSearchCV\n",
    "from sklearn.metrics import classification_report,f1_score\n",
    "from sklearn.impute import SimpleImputer \n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.utils import resample\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split,cross_val_score,StratifiedKFold,RandomizedSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.decomposition import PCA\n",
    "#import xgboost as xgb\n",
    "#from scikit_posthocs import posthoc_nemenyi_friedman\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ee34dd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('heart.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2033f36a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd29f751",
   "metadata": {},
   "outputs": [],
   "source": [
    "a=df.dtypes\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d051714d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d15db1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from the above there is no datatype object that condradicts with the above description\n",
    "\n",
    "# we splitted the dataset based on feature type=categorical and numeric. \n",
    "allFeatures=df.columns[0:len(df.columns) - 1]\n",
    "catFeatures=[]\n",
    "for i in range((len(df.columns)-1)):\n",
    "    if df.dtypes[i]=='O':\n",
    "        print(i)\n",
    "        catFeatures.append(i)\n",
    "catFeatures.append(5) #fasting bs is categorical 1: if FastingBS > 120 mg/dl, 0: otherwise\n",
    "\n",
    "catFeatures=df.columns[catFeatures]\n",
    "print(catFeatures)\n",
    "\n",
    "numFeatures= [i for i in allFeatures if not(i in catFeatures)]\n",
    "print(allFeatures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f27560b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[numFeatures].describe()\n",
    "#get rid of min resting bp or data points that are insignificant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2f5c5c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "s=0\n",
    "c= np.where(df['RestingBP']==0)\n",
    "print(c[0])\n",
    "df.iloc[449]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd41c68b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df.drop(index=449,axis=0) #removed this index because it has resting BP. and Cholestrol zero\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5064f43",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df.reset_index()\n",
    "df=df.drop(['index'],axis=1)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15f445b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams[\"figure.figsize\"] = [8, 5]\n",
    "plt.rcParams[\"figure.autolayout\"] = True\n",
    "df[numFeatures].hist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3806c8ce",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for cat in catFeatures:\n",
    "    fig, ax = plt.subplots()\n",
    "    df[cat].value_counts().plot(ax=ax, kind='bar', xlabel=cat, ylabel='frequency')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ae00ea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in catFeatures:\n",
    "    a=df[i].describe()\n",
    "    print('Describe for '+ str(i))\n",
    "    print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1105629",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "335f2b63",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compute Pearson pairwise correlation of features\n",
    "corr_matrix=df[numFeatures].corr()\n",
    "print(corr_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44a05a50",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visualize Pearson pairwise correlation of features\n",
    "fig = plt.figure()\n",
    "sns.set(rc = {'figure.figsize':(15,8)})\n",
    "ax = sns.heatmap(corr_matrix,cmap='coolwarm', vmin=-1, vmax=1,annot = True)\n",
    "plt.xticks(rotation=90)\n",
    "ax.set_xticklabels(df[numFeatures].columns)\n",
    "ax.set_yticklabels(df[numFeatures].columns)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1f5fbc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cluster = df[allFeatures]\n",
    "df_cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc9460e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import scale\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "df_new = pd.get_dummies(df_cluster, columns=catFeatures)\n",
    "\n",
    "scaled = scale(df_new)\n",
    "pca_model = PCA()\n",
    "pca_model = pca_model.fit_transform(scaled)\n",
    "\n",
    "pca_arr = []\n",
    "\n",
    "for i in range(21):\n",
    "    i = i + 1\n",
    "    string = \"PCA\" + str(i)\n",
    "    pca_arr.append(string)\n",
    "    \n",
    "df_pca = pd.DataFrame(pca_model, columns = pca_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f128fe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "fig, ax1 = plt.subplots()\n",
    "\n",
    "scaled = scale(df_new)\n",
    "\n",
    "y_pred = KMeans(n_clusters = 2, init = 'random', max_iter = 5).fit_predict(scaled)\n",
    "plt.scatter(scaled[:,0], scaled[:,1], c = y_pred, cmap = 'spring')\n",
    "\n",
    "\n",
    "#for i,name in enumerate(df_cluster.index.values):\n",
    "#    ax1.annotate(df_cluster.index[i], (scaled[i,0], scaled[i,1]), ha = 'center', fontsize = 5)\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b0c5c9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ks = list(range(1,11))\n",
    "\n",
    "scores = []\n",
    "\n",
    "for k in ks:\n",
    "    model = KMeans(n_clusters=k)\n",
    "    model.fit_predict(df_new)\n",
    "    scores.append(-model.score(df_new))\n",
    "    \n",
    "plt.plot(ks,scores)\n",
    "plt.ylabel('tital intra-cluster distance')\n",
    "plt.xlabel('k')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "501e0dc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#maybe\n",
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "def plot_decision_regions(X, y, classifier, resolution=0.02):\n",
    "    # setup marker generator and color map\n",
    "    markers = ('s', 'x', 'o', '^', 'v')\n",
    "    colors = ('red', 'blue', 'lightgreen', 'gray', 'cyan')\n",
    "    cmap = ListedColormap(colors[:len(np.unique(y))])\n",
    "\n",
    "    # plot the decision surface\n",
    "    x1_min, x1_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "    x2_min, x2_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "    xx1, xx2 = np.meshgrid(np.arange(x1_min, x1_max, resolution),\n",
    "                           np.arange(x2_min, x2_max, resolution))\n",
    "    Z = classifier.predict(np.array([xx1.ravel(), xx2.ravel()]).T)\n",
    "    Z = Z.reshape(xx1.shape)\n",
    "    plt.contourf(xx1, xx2, Z, alpha=0.4, cmap=cmap)\n",
    "    plt.xlim(xx1.min(), xx1.max())\n",
    "    plt.ylim(xx2.min(), xx2.max())\n",
    "\n",
    "    # plot class samples\n",
    "    for idx, cl in enumerate(np.unique(y)):\n",
    "        plt.scatter(x=X[y == cl, 0], \n",
    "                    y=X[y == cl, 1],\n",
    "                    alpha=0.6, \n",
    "                    c=[cmap(idx)],\n",
    "                    edgecolor='black',\n",
    "                    marker=markers[idx], \n",
    "                    label=cl)# plot decision regions for training set\n",
    "\n",
    "\n",
    "plot_decision_regions(X_train_pca, y_train, classifier=lr)\n",
    "plt.xlabel('PC 1')\n",
    "plt.ylabel('PC 2')\n",
    "plt.legend(loc='lower left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4afea72c",
   "metadata": {},
   "outputs": [],
   "source": [
    "numFeatures.append('HeartDisease')\n",
    "numFeatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa0a16c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_matrix=df[numFeatures].corr()\n",
    "print(corr_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db90c781",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visualize Pearson pairwise correlation of features\n",
    "fig = plt.figure()\n",
    "sns.set(rc = {'figure.figsize':(15,8)})\n",
    "ax = sns.heatmap(corr_matrix,cmap='coolwarm', vmin=-1, vmax=1,annot = True)\n",
    "plt.xticks(rotation=90)\n",
    "ax.set_xticklabels(df[numFeatures].columns)\n",
    "ax.set_yticklabels(df[numFeatures].columns)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6abdafb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove Cholesterol variable\n",
    "df = df.drop(labels = \"Cholesterol\", axis = 1)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae1cc5e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "allFeatures=df.columns[0:len(df.columns)-1]\n",
    "\n",
    "catFeatures=[]\n",
    "for i in range((len(df.columns)-1)):\n",
    "    if df.dtypes[i]=='O':\n",
    "        print(i)\n",
    "        catFeatures.append(i)\n",
    "catFeatures.append(4) #fasting bs is categorical 1: if FastingBS > 120 mg/dl, 0: otherwise\n",
    "\n",
    "catFeatures=df.columns[catFeatures]\n",
    "print(catFeatures)\n",
    "\n",
    "numFeatures= [i for i in allFeatures if not(i in catFeatures)]\n",
    "print(numFeatures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f37c1d2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[allFeatures]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9252a28",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(df[allFeatures], df[\"HeartDisease\"], test_size=0.20, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bf6d0b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(df[allFeatures], df[\"HeartDisease\"], test_size = 0.2, random_state=42, stratify=df[\"HeartDisease\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3feeb893",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_df = pd.DataFrame(y_train)\n",
    "count = 0\n",
    "for i in range(len(y_df)):\n",
    "    if (y_df.values[i].item() == 0):\n",
    "        count = count + 1\n",
    "\n",
    "count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d23fcc1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_df = pd.DataFrame(y_test)\n",
    "count = 0\n",
    "for i in range(len(y_df)):\n",
    "    if (y_df.values[i].item() == 0):\n",
    "        count = count + 1\n",
    "\n",
    "count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61245b9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "328/len(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ae95f7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "82/len(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81fe44f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82ff7ea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.concat([X_train, y_train], axis=1)\n",
    "train = train.reset_index(drop=True)\n",
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "729aec6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 4. Using bootstrapping method, create 50 in-bag (training and validation) with the size of data, \n",
    "# # and related 50 out-of-bag or testing.\n",
    "# samples_in_bag = []\n",
    "# samples_out_bag = []\n",
    "# # There are 733 total samples = 733 indicies\n",
    "# indices = list(range(1, len(train[allFeatures])))\n",
    "\n",
    "# for i in range(5): #FIXME CHANGE BACK TO 50\n",
    "#     out_bag_indicies = []\n",
    "#     if i == 0:\n",
    "#         # all indices in the dataset\n",
    "#         Index=range(0,int(len(train[allFeatures])))\n",
    "#         # Resamples the array of indicies. Random state is needed for the first bootstrap to generate different \n",
    "#         # bootstraps\n",
    "#         S1 = resample(Index, replace=True, n_samples=len(Index), random_state=1)\n",
    "#         # appends to the samples in the array of in bags bootstraps\n",
    "#         samples_in_bag.append(set(S1))\n",
    "#     else:\n",
    "#         # all indices in the dataset\n",
    "#         Index=range(0,int(len(train[allFeatures])))\n",
    "#         # Resamples the array of indicies.\n",
    "#         S1 = resample(Index, replace=True, n_samples=len(Index))\n",
    "#         # appends to the samples in the array of in bags bootstraps\n",
    "#         samples_in_bag.append(set(S1))\n",
    "        \n",
    "#     # places the non-selected indicies into an out of bag bootstrap\n",
    "#     # goes through all the indices\n",
    "#     for j in range(len(indices)):\n",
    "#         if indices[j] not in samples_in_bag[i]:\n",
    "#             out_bag_indicies.append(j)\n",
    "#     # appends to the array of out bags bootstraps\n",
    "#     samples_out_bag.append(out_bag_indicies)\n",
    "    \n",
    "# # out of bag observations percentages of total data\n",
    "# for i in range(5): #FIXME CHANGE BACK TO 50\n",
    "#     print(len(samples_out_bag[i])/len(indices)*100.0) # out of bag \n",
    "#     # prints number of samples in one out of bag bootstrap, and prints number of samples in one in bag bootstrap\n",
    "#     print(len(samples_out_bag[i]),len(samples_in_bag[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91eea9b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1) #Set a random seed for reproducibility\n",
    "inBags = [] \n",
    "outBags = []\n",
    "\n",
    "for i in range(5): #Loop through 50 times to create 50 InBags/OutBags\n",
    "    \n",
    "    curInBag = sklearn.utils.resample(train, replace=True) #Creates a boot strap inBag the length of boston with replacement\n",
    "    \n",
    "    outBagRows = [] #to hold all of the rows not represented in the inBag\n",
    "    for i in range(len(train)): \n",
    "        if i not in np.unique(list(curInBag.index)):  #Check to see if the row is not represented in the current in bag\n",
    "            \n",
    "            outBagRows.append(train.loc[i].to_frame().T) # When I select a row not in the in bag, it is a series\n",
    "                                                           # I Use the .to_frame to convert the series to a DataFrame\n",
    "                                                           # The data Frame is oriented the wrong way so the .T transposes it\n",
    "    \n",
    "    curOutBag = pd.concat(outBagRows) #This concatenates all of the out bag rows into one DF\n",
    "\n",
    "    #This if statment checks to see if every row of boston is represnted between the cur inbag and the cur outbag\n",
    "    if len(np.unique(list(curInBag.index))) + len(np.unique(list(curOutBag.index))) != len(train):\n",
    "        print(\"ERROR! ALl of the unique values should be represented\")\n",
    "\n",
    "    #Add the inbag/outBag to their respective lists\n",
    "    inBags.append(curInBag)\n",
    "    outBags.append(curOutBag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17378a82",
   "metadata": {},
   "outputs": [],
   "source": [
    "inBags[0].columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36ef28ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler() #Creates a SKlearn standard Scaler object\n",
    "\n",
    "for i in range(len(inBags)): #Loop through each of the inBags\n",
    "    \n",
    "    #STANDARD SCALER\n",
    "    scaler.fit(inBags[i][numFeatures]) #Transforms inBags\n",
    "    inBags[i][numFeatures]=scaler.transform(inBags[i][numFeatures])\n",
    "\n",
    "    scaler.fit(outBags[i][numFeatures]) #Transforms outBags\n",
    "    outBags[i][numFeatures]=scaler.transform(outBags[i][numFeatures])\n",
    "    \n",
    "    #ENCODE CATEGORICAL VARIABLES\n",
    "    from sklearn import preprocessing\n",
    "#     enc = OneHotEncoder(sparse=False)\n",
    "#     inBags[i] = enc.fit_transform(inBags[i][catFeatures]])\n",
    "    \n",
    "    inBags[i] = pd.get_dummies(inBags[i], columns=catFeatures)  #Transform inBags\n",
    "    outBags[i] = pd.get_dummies(outBags[i], columns=catFeatures)  #Transform outBags\n",
    "\n",
    "\n",
    "#     le = preprocessing.LabelEncoder()\n",
    "#     le.fit_transform(inBags[i][xFeatures])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9fd3b44",
   "metadata": {},
   "outputs": [],
   "source": [
    "xFeatures = inBags[0].columns.tolist()\n",
    "xFeatures.remove('HeartDisease')\n",
    "yFeatures = ['HeartDisease']\n",
    "\n",
    "#catFeatures\n",
    "inBags[0][xFeatures]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f8e01bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Use sklearn StandardScaler to scale numeric features\n",
    "# # Standardized features by removing the mean and scaling to unit variance\n",
    "# array_of_pds_train = []\n",
    "# array_of_pds_test = []\n",
    "# train_y_arr = []\n",
    "# test_y_arr = []\n",
    "\n",
    "# for i in range(len(samples_in_bag)):\n",
    "#     # get the one in bag bootstrap and one out of bag bootstrap\n",
    "#     sample_in = samples_in_bag[i]\n",
    "#     sample_out = samples_out_bag[i]\n",
    "\n",
    "#     # create scalor object\n",
    "#     scalor=StandardScaler()\n",
    "    \n",
    "#     # select the subset from data that corresponds to specific in bag bootstrap\n",
    "#     df_train = train[numFeatures].loc[sample_in, :]\n",
    "#     # select the subset from data that corresponds to specific out of bag bootstrap\n",
    "#     df_test = train[numFeatures].loc[sample_out, :]\n",
    "#     # select the subset of the categorical variable that corresponds to specific in bag Bootstrap \n",
    "#     df_train_char = train[catFeatures].loc[sample_in]\n",
    "#     df_train_char = df_train_char.reset_index(drop=True)\n",
    "#     # select the subset of the categorical variable that corresponds to specific out of bag Bootstrap \n",
    "#     df_test_char = train[catFeatures].loc[sample_out]\n",
    "#     df_test_char = df_test_char.reset_index(drop = True)\n",
    "#     # select the subset of the target that corresponds to specific in bag bootstrap\n",
    "#     train_y = train['HeartDisease'].loc[sample_in].to_numpy()\n",
    "#     # select the subset of the target that corresponds to specific out of bag bootstrap\n",
    "#     test_y = train['HeartDisease'].loc[sample_out].to_numpy()\n",
    "    \n",
    "#     # Computes the mean and std of the data\n",
    "#     scalor.fit(df_train)\n",
    "#     # Performs standardization on the training set and testing set\n",
    "#     df_train=scalor.transform(df_train)\n",
    "#     df_test=scalor.transform(df_test)\n",
    "#     #places resulting values into a dataframe\n",
    "#     df_train = pd.DataFrame(df_train, columns=numFeatures)\n",
    "#     df_test = pd.DataFrame(df_test, columns=numFeatures)\n",
    "    \n",
    "#     # attach the categorical variables to the dataframes\n",
    "#     df_train_allF = pd.concat([df_train, df_train_char], axis=1)\n",
    "#     df_test_allF = pd.concat([df_test, df_test_char], axis = 1)\n",
    "#     #df_train[catFeatures] = df_train_char\n",
    "#     #df_test[catFeatures] = df_test_char\n",
    "    \n",
    "#     # creates an array of training dataframes that have subsets from each bootstrap\n",
    "#     array_of_pds_train.append(df_train_allF)\n",
    "#     # creates an array of testing dataframes that have subsets from each bootstrap\n",
    "#     array_of_pds_test.append(df_test_allF)\n",
    "#     # creates an array of y values for the training data that have subsets from each bootstrap\n",
    "#     train_y_arr.append(train_y)\n",
    "#     # creates an array of y values for the testing data that have subsets from each bootstrap\n",
    "#     test_y_arr.append(test_y) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2d0a4ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Encode all possible categorical data using get_dummies\n",
    "# for i in range(len(array_of_pds_train)):\n",
    "#     array_of_pds_train[i] = pd.get_dummies(array_of_pds_train[i])\n",
    "#     array_of_pds_test[i] = pd.get_dummies(array_of_pds_test[i])\n",
    "\n",
    "# array_of_pds_test[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe3db1b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_inner = StratifiedKFold(n_splits=5, shuffle=True, random_state=2)\n",
    "\n",
    "model_rf=RandomForestClassifier(random_state=1)\n",
    "rf_pipeline = Pipeline([ \n",
    "    #('pca', PCA()), \n",
    "    ('model_rf',model_rf)\n",
    "])\n",
    "rf_param = {\n",
    "               #'pca__n_components': [0.60,0.70,0.80,0.90],\n",
    "               'model_rf__bootstrap': [True, False],\n",
    "               'model_rf__max_depth': [2, 3, 5, 7, 9],\n",
    "               'model_rf__max_features': [10],\n",
    "               'model_rf__n_estimators': [10, 100, 500]} \n",
    "\n",
    "rf_random = RandomizedSearchCV(rf_pipeline, rf_param, cv=cv_inner, scoring='f1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a801483a",
   "metadata": {},
   "outputs": [],
   "source": [
    "rfModels = []\n",
    "rfScores = []\n",
    "\n",
    "# for i in range(len(samples_in_bag)):\n",
    "#     model = rf_random.fit(array_of_pds_train[i], np.asarray(df.loc[samples_in_bag[i]][\"HeartDisease\"].squeeze().tolist()))\n",
    "#     rfModels.append(model)\n",
    "#     curScore = f1_score(df.loc[samples_out_bag[i]][\"HeartDisease\"], model.predict(array_of_pds_test[i]))\n",
    "#     rfScores.append(curScore)\n",
    "\n",
    "\n",
    "for i in range(len(inBags)):\n",
    "    model = rf_random.fit(inBags[i][xFeatures], np.asarray(inBags[i][yFeatures].squeeze().tolist()))\n",
    "    rfModels.append(model)\n",
    "    curScore = f1_score(outBags[i][yFeatures].squeeze().tolist(), model.predict(outBags[i][xFeatures]))\n",
    "    rfScores.append(curScore)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd4b8003",
   "metadata": {},
   "outputs": [],
   "source": [
    "topScore = max(rfScores)\n",
    "topModel = rfModels[0]\n",
    "for i in range(len(rfScores)):\n",
    "    if rfScores[i] == topScore:\n",
    "        topModel = rfModels[i]\n",
    "bestModel = topModel.best_estimator_.named_steps[\"model_rf\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25b2c996",
   "metadata": {},
   "outputs": [],
   "source": [
    "topScore = max(rfScores)\n",
    "topModel = rfModels[0]\n",
    "for i in range(len(rfScores)):\n",
    "    if rfScores[i] == topScore:\n",
    "        topModel = rfModels[i]\n",
    "        \n",
    "bestModel = topModel.best_estimator_.named_steps[\"model_rf\"]\n",
    "importances = bestModel.feature_importances_\n",
    "forest_importances = pd.Series(importances, index=xFeatures)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "forest_importances.plot.bar(ax=ax)\n",
    "ax.set_title(\"Feature importances using MDI\")\n",
    "ax.set_ylabel(\"Mean decrease in impurity\")\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea8fe20f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_xgb = xgb.XGBClassifier(random_state=1,objective='binary:logistic',eval_metric='logloss',use_label_encoder=False)\n",
    "xgb_pipeline = Pipeline([\n",
    "    ('model_xgb', model_xgb)\n",
    "])\n",
    "\n",
    "xgb_param= {\n",
    "    'model_xgb__max_depth': [2, 3, 5,7,9],\n",
    "    'model_xgb__n_estimators': [10, 100, 500]}\n",
    "xgb_random = RandomizedSearchCV(xgb_pipeline, xgb_param, cv=cv_inner, scoring='f1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "652287d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "xgbModels = []\n",
    "xgbScores = []\n",
    "\n",
    "for i in range(len(inBags)):\n",
    "    model = xgb_random.fit(inBags[i][xFeatures], np.asarray(inBags[i][yFeatures].squeeze().tolist()))\n",
    "    xgbModels.append(model)\n",
    "    curScore = f1_score(outBags[i][yFeatures].squeeze().tolist(), model.predict(outBags[i][xFeatures]))\n",
    "    xgbScores.append(curScore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6be862a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import plot_importance \n",
    "\n",
    "topScore = max(xgbScores)\n",
    "topModel = xgbModels[0]\n",
    "for i in range(len(xgbModels)):\n",
    "    if xgbScores[i] == topScore:\n",
    "        topModel = xgbModels[i]\n",
    "\n",
    "bestModel = topModel.best_estimator_.named_steps[\"model_xgb\"]\n",
    "\n",
    "plot_importance(bestModel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "801ff549",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "\n",
    "\n",
    "model_svm=svm.SVC()\n",
    "svm_pipeline = Pipeline([ \n",
    "    ('model_svm',model_svm)\n",
    "])\n",
    "svm_param = {\n",
    "               'model_svm__kernel': ['linear'],\n",
    "               'model_svm__gamma': ['scale', 'auto'],\n",
    "               'model_svm__C': [.1, .5, 1, 1.5, 2]}\n",
    "\n",
    "svm_random = RandomizedSearchCV(svm_pipeline, svm_param, cv=cv_inner, scoring='f1', random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bd3dfa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "svmModels = []\n",
    "svmScores = []\n",
    "\n",
    "for i in range(len(inBags)):\n",
    "    model = svm_random.fit(inBags[i][xFeatures], np.asarray(inBags[i][yFeatures].squeeze().tolist()))\n",
    "    svmModels.append(model)\n",
    "    curScore = f1_score(outBags[i][yFeatures].squeeze().tolist(), model.predict(outBags[i][xFeatures]))\n",
    "    svmScores.append(curScore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f08a117f",
   "metadata": {},
   "outputs": [],
   "source": [
    "topScore = max(svmScores)\n",
    "topModel = svmModels[0]\n",
    "for i in range(len(svmScores)):\n",
    "    if svmScores[i] == topScore:\n",
    "        topModel = svmModels[i]\n",
    "        \n",
    "bestModel = topModel.best_estimator_.named_steps[\"model_svm\"]\n",
    "\n",
    "importances = bestModel.coef_\n",
    "\n",
    "forest_importances = pd.Series(importances[0], index=xFeatures)\n",
    "fig, ax = plt.subplots()\n",
    "forest_importances.plot.bar(ax=ax)\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b54f4150",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Friedman\n",
    "stats.friedmanchisquare(svmScores,xgbScores,rfScores) #Gets the Friedman statistic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bd3aa55",
   "metadata": {},
   "outputs": [],
   "source": [
    "trans_groups=np.array([KNNScores,LinRegScores,DTScores,SVMScores,RandomForestScores]).T #Merges all the scores into one dataframe \n",
    "posthoc_nemenyi_friedman(trans_groups) #Gets the pairwise comparison of all the scores. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11f208c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#**********************\n",
    "#        todo\n",
    "#**********************\n",
    "\n",
    "#one-hot-encoding (Sam)\n",
    "#Some clustering algorithm for exploring the data (KNN) (Marko)\n",
    "#Shap (Nidhi)\n",
    "#add another evaluation metric (not f1) (Nidhi)\n",
    "#add another ML algorithm that we haven't done in class (Sam)\n",
    "#comment and clean code. (Marko)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
